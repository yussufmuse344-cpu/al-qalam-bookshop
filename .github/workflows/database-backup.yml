name: Nightly Database Backup

on:
  schedule:
    # Runs every day at 2:00 AM UTC (adjust timezone as needed)
    - cron: "0 2 * * *"
  workflow_dispatch: # Allows manual trigger from GitHub Actions tab

jobs:
  backup:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Supabase CLI
  run: |
    curl -fsSL https://cli.supabase.com/install.sh | sh -s -- -b /usr/local/bin


      - name: Login to Supabase
        env:
          SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
        run: |
          supabase login --token $SUPABASE_ACCESS_TOKEN

      - name: Create backup directory
        run: |
          mkdir -p backups
          echo "BACKUP_DATE=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_ENV

      - name: Export database schema
        env:
          SUPABASE_PROJECT_ID: ${{ secrets.SUPABASE_PROJECT_ID }}
          SUPABASE_DB_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
        run: |
          # Export database schema
          supabase db dump --project-id $SUPABASE_PROJECT_ID --password $SUPABASE_DB_PASSWORD > backups/schema_${{ env.BACKUP_DATE }}.sql

          # Export data only
          supabase db dump --project-id $SUPABASE_PROJECT_ID --password $SUPABASE_DB_PASSWORD --data-only > backups/data_${{ env.BACKUP_DATE }}.sql

          # Export full database (schema + data)
          supabase db dump --project-id $SUPABASE_PROJECT_ID --password $SUPABASE_DB_PASSWORD --use-copy > backups/full_backup_${{ env.BACKUP_DATE }}.sql

      - name: Compress backup files
        run: |
          cd backups
          tar -czf backup_${{ env.BACKUP_DATE }}.tar.gz *.sql
          rm *.sql
          cd ..

      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ env.BACKUP_DATE }}
          path: backups/backup_${{ env.BACKUP_DATE }}.tar.gz
          retention-days: 30 # Keep backups for 30 days

      - name: Create backup report
        run: |
          echo "## Database Backup Report" > backup_report.md
          echo "" >> backup_report.md
          echo "**Date:** $(date +'%Y-%m-%d %H:%M:%S UTC')" >> backup_report.md
          echo "**Status:** ✅ Success" >> backup_report.md
          echo "" >> backup_report.md
          echo "### Files Created:" >> backup_report.md
          echo "- Schema backup" >> backup_report.md
          echo "- Data backup" >> backup_report.md
          echo "- Full backup (compressed)" >> backup_report.md
          echo "" >> backup_report.md
          echo "### Backup Details:" >> backup_report.md
          ls -lh backups/ >> backup_report.md

      - name: Upload backup report
        uses: actions/upload-artifact@v4
        with:
          name: backup-report-${{ env.BACKUP_DATE }}
          path: backup_report.md
          retention-days: 30

      # Optional: Upload to external storage (uncomment and configure as needed)
      # - name: Upload to AWS S3
      #   env:
      #     AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     AWS_REGION: us-east-1
      #   run: |
      #     aws s3 cp backups/backup_${{ env.BACKUP_DATE }}.tar.gz s3://your-backup-bucket/hassan-bookshop/

      # Optional: Send notification on failure
      - name: Notify on failure
        if: failure()
        run: |
          echo "❌ Backup failed! Check the logs for details."
          # Add notification logic here (email, Slack, Discord, etc.)

  # Optional: Cleanup old artifacts (runs weekly)
  cleanup:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' && github.event.schedule == '0 3 * * 0'

    steps:
      - name: Cleanup old artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
            });

            const oldArtifacts = artifacts.data.artifacts.filter(artifact => {
              const artifactDate = new Date(artifact.created_at);
              const thirtyDaysAgo = new Date();
              thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);
              return artifactDate < thirtyDaysAgo && artifact.name.includes('database-backup');
            });

            for (const artifact of oldArtifacts) {
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
            }
